{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5157181",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mReddit Comment Clusterer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mwith cluster assignments to a new CSV file.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Set environment variable to fix threadpoolctl issue\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENBLAS_NUM_THREADS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMKL_NUM_THREADS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reddit Comment Clusterer\n",
    "\n",
    "This script takes a CSV file of Reddit comments (output from reddit_nested_comments_scraper.py)\n",
    "and clusters them by topics using unsupervised machine learning. It exports the results\n",
    "with cluster assignments to a new CSV file.\n",
    "\"\"\"\n",
    "\n",
    "# Set environment variable to fix threadpoolctl issue\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "class RedditCommentClusterer:\n",
    "    def __init__(self, min_clusters: int = 2, max_clusters: int = 15, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the comment clusterer.\n",
    "        \n",
    "        Args:\n",
    "            min_clusters: Minimum number of clusters to try\n",
    "            max_clusters: Maximum number of clusters to try\n",
    "            random_state: Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_clusters = max_clusters\n",
    "        self.random_state = random_state\n",
    "        self.vectorizer = None\n",
    "        self.kmeans = None\n",
    "        self.optimal_k = None\n",
    "        self.feature_matrix = None\n",
    "        self.cluster_labels = None\n",
    "        self.topic_keywords = {}\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and preprocess text for clustering.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw comment text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        if text is None or pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            # Convert to lowercase\n",
    "            text = str(text).lower()\n",
    "            \n",
    "            # Remove URLs\n",
    "            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "            \n",
    "            # Remove Reddit-specific formatting\n",
    "            text = re.sub(r'/u/\\w+', '', text)  # Remove user mentions\n",
    "            text = re.sub(r'/r/\\w+', '', text)  # Remove subreddit mentions\n",
    "            text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)  # Remove bold formatting\n",
    "            text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)  # Remove italic formatting\n",
    "            \n",
    "            # Remove special characters but keep spaces and basic punctuation\n",
    "            text = re.sub(r'[^\\w\\s.,!?-]', ' ', text)\n",
    "            \n",
    "            # Remove extra whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            return ''\n",
    "    \n",
    "    def load_comments_from_csv(self, csv_file: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load comments from the nested CSV format created by the scraper.\n",
    "        \n",
    "        Args:\n",
    "            csv_file: Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with processed comments\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Extract comment text from the level columns\n",
    "            level_columns = [col for col in df.columns if col.startswith('level_')]\n",
    "            \n",
    "            # Combine all comment text from different nesting levels\n",
    "            comments = []\n",
    "            for idx, row in df.iterrows():\n",
    "                comment_text = ''\n",
    "                \n",
    "                for col in level_columns:\n",
    "                    cell_value = row[col]\n",
    "                    \n",
    "                    if pd.notna(cell_value) and cell_value != '':\n",
    "                        comment_text = cell_value\n",
    "                        break\n",
    "                \n",
    "                if comment_text and str(comment_text).strip():\n",
    "                    comment_data = {\n",
    "                        'original_index': idx,\n",
    "                        'comment_text': comment_text,\n",
    "                        'author': row.get('author', 'unknown'),\n",
    "                        'score': row.get('score', 0),\n",
    "                        'created_utc': row.get('created_utc', 0),\n",
    "                        'comment_id': row.get('comment_id', ''),\n",
    "                        'parent_id': row.get('parent_id', ''),\n",
    "                        'depth': self._get_comment_depth(row, level_columns)\n",
    "                    }\n",
    "                    comments.append(comment_data)\n",
    "            \n",
    "            return pd.DataFrame(comments)\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    def _get_comment_depth(self, row: pd.Series, level_columns: List[str]) -> int:\n",
    "        \"\"\"Get the depth/nesting level of a comment.\"\"\"\n",
    "        for i, col in enumerate(level_columns):\n",
    "            if pd.notna(row[col]) and row[col] != '':\n",
    "                return i\n",
    "        return 0\n",
    "    \n",
    "    def vectorize_comments(self, comments: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert comment text to TF-IDF feature vectors.\n",
    "        \n",
    "        Args:\n",
    "            comments: List of preprocessed comment texts\n",
    "            \n",
    "        Returns:\n",
    "            TF-IDF feature matrix\n",
    "        \"\"\"\n",
    "        # Filter out empty comments\n",
    "        valid_comments = []\n",
    "        for comment in comments:\n",
    "            if comment and str(comment).strip():\n",
    "                valid_comments.append(str(comment))\n",
    "        \n",
    "        if len(valid_comments) < 2:\n",
    "            raise ValueError(\"Need at least 2 valid comments for clustering\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize TF-IDF vectorizer\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=1000,  # Limit to top 1000 features\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "                min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "                max_df=0.95  # Ignore terms that appear in more than 95% of documents\n",
    "            )\n",
    "            \n",
    "            # Fit and transform the comments\n",
    "            self.feature_matrix = self.vectorizer.fit_transform(valid_comments)\n",
    "            \n",
    "            return self.feature_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    def find_optimal_clusters(self, feature_matrix: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Find optimal number of clusters using elbow method and silhouette analysis.\n",
    "        \n",
    "        Args:\n",
    "            feature_matrix: TF-IDF feature matrix\n",
    "            \n",
    "        Returns:\n",
    "            Optimal number of clusters\n",
    "        \"\"\"\n",
    "        try:\n",
    "            max_k = min(self.max_clusters, feature_matrix.shape[0] - 1)\n",
    "            \n",
    "            if max_k < self.min_clusters:\n",
    "                print(f\"Warning: Not enough comments for clustering. Using {max_k} clusters.\")\n",
    "                return max_k\n",
    "            \n",
    "            inertias = []\n",
    "            silhouette_scores = []\n",
    "            k_range = range(self.min_clusters, max_k + 1)\n",
    "            \n",
    "            for k in k_range:\n",
    "                try:\n",
    "                    # Use simplified KMeans parameters to avoid threadpoolctl issues\n",
    "                    kmeans = KMeans(\n",
    "                        n_clusters=k, \n",
    "                        random_state=self.random_state, \n",
    "                        n_init=5,  # Reduced from 10\n",
    "                        max_iter=100,  # Reduced from 300\n",
    "                        algorithm='lloyd'  # Explicitly set algorithm\n",
    "                    )\n",
    "                    \n",
    "                    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "                    \n",
    "                    inertias.append(kmeans.inertia_)\n",
    "                    \n",
    "                    if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette score\n",
    "                        sil_score = silhouette_score(feature_matrix, cluster_labels)\n",
    "                        silhouette_scores.append(sil_score)\n",
    "                    else:\n",
    "                        silhouette_scores.append(0)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    # Continue with next k value instead of failing completely\n",
    "                    inertias.append(float('inf'))\n",
    "                    silhouette_scores.append(0)\n",
    "                    continue\n",
    "            \n",
    "            # Find elbow point\n",
    "            optimal_k = self._find_elbow_point(k_range, inertias)\n",
    "            \n",
    "            # Validate with silhouette score\n",
    "            if silhouette_scores and max(silhouette_scores) > 0:\n",
    "                best_sil_k = k_range[np.argmax(silhouette_scores)]\n",
    "                print(f\"Elbow method suggests {optimal_k} clusters\")\n",
    "                print(f\"Best silhouette score at {best_sil_k} clusters ({max(silhouette_scores):.3f})\")\n",
    "                \n",
    "                # Use silhouette score if significantly better\n",
    "                if max(silhouette_scores) > 0.3 and abs(best_sil_k - optimal_k) <= 2:\n",
    "                    optimal_k = best_sil_k\n",
    "            \n",
    "            return optimal_k\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to a reasonable number\n",
    "            return min(5, feature_matrix.shape[0] - 1)\n",
    "    \n",
    "    def _find_elbow_point(self, k_range: range, inertias: List[float]) -> int:\n",
    "        \"\"\"Find the elbow point in the inertia curve.\"\"\"        \n",
    "        try:\n",
    "            # Filter out infinite values\n",
    "            valid_inertias = [(k, inertia) for k, inertia in zip(k_range, inertias) if inertia != float('inf')]\n",
    "            \n",
    "            if len(valid_inertias) < 3:\n",
    "                return list(k_range)[0]\n",
    "            \n",
    "            k_values, inertia_values = zip(*valid_inertias)\n",
    "            \n",
    "            # Calculate the rate of change\n",
    "            changes = []\n",
    "            for i in range(1, len(inertia_values)):\n",
    "                change = inertia_values[i-1] - inertia_values[i]\n",
    "                changes.append(change)\n",
    "            \n",
    "            # Find the point where the rate of change decreases most\n",
    "            if len(changes) > 1:\n",
    "                second_changes = []\n",
    "                for i in range(1, len(changes)):\n",
    "                    second_change = changes[i-1] - changes[i]\n",
    "                    second_changes.append(second_change)\n",
    "                \n",
    "                if second_changes:\n",
    "                    elbow_idx = np.argmax(second_changes) + 2  # +2 because of indexing offset\n",
    "                    result_idx = min(elbow_idx, len(k_values) - 1)\n",
    "                    return k_values[result_idx]\n",
    "            \n",
    "            # Fallback: use middle of valid range\n",
    "            return k_values[len(k_values) // 2]\n",
    "            \n",
    "        except Exception as e:\n",
    "            return list(k_range)[0]\n",
    "    \n",
    "    def cluster_comments(self, feature_matrix: np.ndarray, n_clusters: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform K-means clustering on the feature matrix.\n",
    "        \n",
    "        Args:\n",
    "            feature_matrix: TF-IDF feature matrix\n",
    "            n_clusters: Number of clusters (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            Cluster labels\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            if n_clusters is None:\n",
    "                n_clusters = self.find_optimal_clusters(feature_matrix)\n",
    "            \n",
    "            self.optimal_k = n_clusters\n",
    "            \n",
    "            # Perform K-means clustering with simplified parameters\n",
    "            self.kmeans = KMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                n_init=5,  # Reduced\n",
    "                max_iter=100,  # Reduced\n",
    "                algorithm='lloyd'  # Explicitly set\n",
    "            )\n",
    "            \n",
    "            self.cluster_labels = self.kmeans.fit_predict(feature_matrix)\n",
    "            \n",
    "            return self.cluster_labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    def extract_topic_keywords(self, n_keywords: int = 10) -> Dict[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Extract top keywords for each cluster to understand topics.\n",
    "        \n",
    "        Args:\n",
    "            n_keywords: Number of top keywords per cluster\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping cluster ID to list of keywords\n",
    "        \"\"\"\n",
    "        if self.kmeans is None or self.vectorizer is None:\n",
    "            raise ValueError(\"Must run clustering before extracting keywords\")\n",
    "        \n",
    "        try:\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            cluster_centers = self.kmeans.cluster_centers_\n",
    "            \n",
    "            for cluster_id in range(len(cluster_centers)):\n",
    "                # Get top feature indices for this cluster\n",
    "                top_indices = cluster_centers[cluster_id].argsort()[-n_keywords:][::-1]\n",
    "                top_keywords = [str(feature_names[i]) for i in top_indices]\n",
    "                self.topic_keywords[cluster_id] = top_keywords\n",
    "            \n",
    "            return self.topic_keywords\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    def create_results_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create results DataFrame with cluster assignments and topic keywords.\n",
    "        \n",
    "        Args:\n",
    "            df: Original comments DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cluster information\n",
    "        \"\"\"\n",
    "        results_df = df.copy()\n",
    "        \n",
    "        # Add cluster assignments\n",
    "        if len(self.cluster_labels) == len(df):\n",
    "            results_df['cluster_id'] = self.cluster_labels\n",
    "        else:\n",
    "            # Handle case where some comments were filtered out\n",
    "            results_df['cluster_id'] = -1\n",
    "            valid_idx = 0\n",
    "            for idx, row in results_df.iterrows():\n",
    "                if row['comment_text'] and str(row['comment_text']).strip():\n",
    "                    if valid_idx < len(self.cluster_labels):\n",
    "                        results_df.loc[idx, 'cluster_id'] = self.cluster_labels[valid_idx]\n",
    "                        valid_idx += 1\n",
    "        \n",
    "        # Add topic keywords\n",
    "        results_df['topic_keywords'] = results_df['cluster_id'].apply(\n",
    "            lambda x: ', '.join(self.topic_keywords.get(x, ['unknown'])) if x >= 0 else 'filtered'\n",
    "        )\n",
    "        \n",
    "        # Add preprocessed text for reference\n",
    "        results_df['preprocessed_text'] = results_df['comment_text'].apply(self.preprocess_text)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def export_results(self, results_df: pd.DataFrame, output_file: str):\n",
    "        \"\"\"\n",
    "        Export clustering results to CSV.\n",
    "        \n",
    "        Args:\n",
    "            results_df: Results DataFrame\n",
    "            output_file: Output CSV file path\n",
    "        \"\"\"\n",
    "        # Reorder columns for better readability\n",
    "        column_order = [\n",
    "            'cluster_id', 'topic_keywords', 'comment_text', 'preprocessed_text',\n",
    "            'author', 'score', 'depth', 'created_utc', 'comment_id', 'parent_id', 'original_index'\n",
    "        ]\n",
    "        \n",
    "        # Only include columns that exist\n",
    "        available_columns = [col for col in column_order if col in results_df.columns]\n",
    "        export_df = results_df[available_columns]\n",
    "        \n",
    "        export_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nClustering Results Summary:\")\n",
    "        print(f\"Total comments processed: {len(results_df)}\")\n",
    "        print(f\"Number of clusters: {self.optimal_k}\")\n",
    "        print(f\"Results exported to: {output_file}\")\n",
    "        \n",
    "        cluster_counts = results_df['cluster_id'].value_counts().sort_index()\n",
    "        print(f\"\\nComments per cluster:\")\n",
    "        for cluster_id, count in cluster_counts.items():\n",
    "            if cluster_id >= 0:\n",
    "                keywords = ', '.join(self.topic_keywords.get(cluster_id, ['unknown'])[:5])\n",
    "                print(f\"  Cluster {cluster_id}: {count} comments (Keywords: {keywords})\")\n",
    "\n",
    "\n",
    "# Main execution - no arguments needed\n",
    "try:\n",
    "    print(\"Loading comments from: accessibility.csv\")\n",
    "    \n",
    "    # Initialize clusterer with hardcoded values\n",
    "    clusterer = RedditCommentClusterer(\n",
    "        min_clusters=2,\n",
    "        max_clusters=15\n",
    "    )\n",
    "    \n",
    "    # Load and preprocess comments\n",
    "    df = clusterer.load_comments_from_csv(\"accessibility.csv\")\n",
    "    \n",
    "    if len(df) < 2:\n",
    "        print(\"Error: Need at least 2 comments for clustering.\")\n",
    "        raise ValueError(\"Not enough comments\")\n",
    "    \n",
    "    print(f\"Loaded {len(df)} comments\")\n",
    "    \n",
    "    # Preprocess and vectorize\n",
    "    preprocessed_comments = []\n",
    "    for i, text in enumerate(df['comment_text']):\n",
    "        processed = clusterer.preprocess_text(text)\n",
    "        preprocessed_comments.append(processed)\n",
    "    \n",
    "    feature_matrix = clusterer.vectorize_comments(preprocessed_comments)\n",
    "    \n",
    "    print(f\"Created feature matrix with {feature_matrix.shape[1]} features\")\n",
    "    \n",
    "    # Perform clustering (no clusters specified, will auto-detect)\n",
    "    cluster_labels = clusterer.cluster_comments(feature_matrix, None)\n",
    "    \n",
    "    # Extract topic keywords (10 keywords per cluster)\n",
    "    topic_keywords = clusterer.extract_topic_keywords(10)\n",
    "    \n",
    "    # Create and export results\n",
    "    results_df = clusterer.create_results_dataframe(df)\n",
    "    clusterer.export_results(results_df, \"reddit_comments_clustered.csv\")\n",
    "    \n",
    "    print(\"Clustering completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
