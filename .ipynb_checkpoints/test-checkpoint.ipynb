{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5157181",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reddit Comment Clusterer\n",
    "\n",
    "This script takes a CSV file of Reddit comments (output from reddit_nested_comments_scraper.py)\n",
    "and clusters them by topics using unsupervised machine learning. It exports the results\n",
    "with cluster assignments to a new CSV file.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class RedditCommentClusterer:\n",
    "    def __init__(self, min_clusters: int = 2, max_clusters: int = 15, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the comment clusterer.\n",
    "        \n",
    "        Args:\n",
    "            min_clusters: Minimum number of clusters to try\n",
    "            max_clusters: Maximum number of clusters to try\n",
    "            random_state: Random state for reproducibility\n",
    "        \"\"\"\n",
    "        self.min_clusters = min_clusters\n",
    "        self.max_clusters = max_clusters\n",
    "        self.random_state = random_state\n",
    "        self.vectorizer = None\n",
    "        self.kmeans = None\n",
    "        self.optimal_k = None\n",
    "        self.feature_matrix = None\n",
    "        self.cluster_labels = None\n",
    "        self.topic_keywords = {}\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and preprocess text for clustering.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw comment text\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text\n",
    "        \"\"\"\n",
    "        logging.debug(f\"preprocess_text called with: {repr(text)} (type: {type(text)})\")\n",
    "        \n",
    "        if text is None or pd.isna(text) or text == '':\n",
    "            logging.debug(\"Text is None, NaN, or empty - returning empty string\")\n",
    "            return ''\n",
    "        \n",
    "        try:\n",
    "            # Convert to lowercase\n",
    "            logging.debug(\"Converting to lowercase\")\n",
    "            text = text.lower()\n",
    "            \n",
    "            # Remove URLs\n",
    "            logging.debug(\"Removing URLs\")\n",
    "            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "            \n",
    "            # Remove Reddit-specific formatting\n",
    "            logging.debug(\"Removing Reddit formatting\")\n",
    "            text = re.sub(r'/u/\\w+', '', text)  # Remove user mentions\n",
    "            text = re.sub(r'/r/\\w+', '', text)  # Remove subreddit mentions\n",
    "            text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)  # Remove bold formatting\n",
    "            text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)  # Remove italic formatting\n",
    "            \n",
    "            # Remove special characters but keep spaces and basic punctuation\n",
    "            logging.debug(\"Removing special characters\")\n",
    "            text = re.sub(r'[^\\w\\s.,!?-]', ' ', text)\n",
    "            \n",
    "            # Remove extra whitespace\n",
    "            logging.debug(\"Removing extra whitespace\")\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            logging.debug(f\"Preprocessed text result: {repr(text)}\")\n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in preprocess_text: {e}, input was: {repr(text)} (type: {type(text)})\")\n",
    "            raise\n",
    "    \n",
    "    def load_comments_from_csv(self, csv_file: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load comments from the nested CSV format created by the scraper.\n",
    "        \n",
    "        Args:\n",
    "            csv_file: Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with processed comments\n",
    "        \"\"\"\n",
    "        logging.info(f\"Loading CSV file: {csv_file}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            logging.info(f\"CSV loaded successfully. Shape: {df.shape}\")\n",
    "            logging.debug(f\"CSV columns: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Extract comment text from the level columns\n",
    "            level_columns = [col for col in df.columns if col.startswith('level_')]\n",
    "            logging.info(f\"Found level columns: {level_columns}\")\n",
    "            \n",
    "            # Combine all comment text from different nesting levels\n",
    "            comments = []\n",
    "            for idx, row in df.iterrows():\n",
    "                logging.debug(f\"Processing row {idx}\")\n",
    "                comment_text = ''\n",
    "                \n",
    "                for col in level_columns:\n",
    "                    cell_value = row[col]\n",
    "                    logging.debug(f\"  Checking column {col}: {repr(cell_value)} (type: {type(cell_value)})\")\n",
    "                    \n",
    "                    if pd.notna(cell_value) and cell_value != '':\n",
    "                        comment_text = cell_value\n",
    "                        logging.debug(f\"  Found comment text in {col}: {repr(comment_text)}\")\n",
    "                        break\n",
    "                \n",
    "                if comment_text and str(comment_text).strip():\n",
    "                    logging.debug(f\"  Adding comment: {repr(comment_text)}\")\n",
    "                    comment_data = {\n",
    "                        'original_index': idx,\n",
    "                        'comment_text': comment_text,\n",
    "                        'author': row.get('author', 'unknown'),\n",
    "                        'score': row.get('score', 0),\n",
    "                        'created_utc': row.get('created_utc', 0),\n",
    "                        'comment_id': row.get('comment_id', ''),\n",
    "                        'parent_id': row.get('parent_id', ''),\n",
    "                        'depth': self._get_comment_depth(row, level_columns)\n",
    "                    }\n",
    "                    comments.append(comment_data)\n",
    "                else:\n",
    "                    logging.debug(f\"  Skipping row {idx} - no valid comment text\")\n",
    "            \n",
    "            logging.info(f\"Extracted {len(comments)} valid comments from {len(df)} rows\")\n",
    "            return pd.DataFrame(comments)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading CSV file: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _get_comment_depth(self, row: pd.Series, level_columns: List[str]) -> int:\n",
    "        \"\"\"Get the depth/nesting level of a comment.\"\"\"\n",
    "        for i, col in enumerate(level_columns):\n",
    "            if pd.notna(row[col]) and row[col] != '':\n",
    "                return i\n",
    "        return 0\n",
    "    \n",
    "    def vectorize_comments(self, comments: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert comment text to TF-IDF feature vectors.\n",
    "        \n",
    "        Args:\n",
    "            comments: List of preprocessed comment texts\n",
    "            \n",
    "        Returns:\n",
    "            TF-IDF feature matrix\n",
    "        \"\"\"\n",
    "        logging.info(f\"vectorize_comments called with {len(comments)} comments\")\n",
    "        \n",
    "        # Filter out empty comments\n",
    "        logging.debug(\"Filtering out empty comments\")\n",
    "        valid_comments = []\n",
    "        for i, comment in enumerate(comments):\n",
    "            logging.debug(f\"Comment {i}: {repr(comment)} (type: {type(comment)})\")\n",
    "            if comment and str(comment).strip():\n",
    "                valid_comments.append(comment)\n",
    "            else:\n",
    "                logging.debug(f\"  Filtered out comment {i}\")\n",
    "        \n",
    "        logging.info(f\"After filtering: {len(valid_comments)} valid comments\")\n",
    "        \n",
    "        if len(valid_comments) < 2:\n",
    "            raise ValueError(\"Need at least 2 valid comments for clustering\")\n",
    "        \n",
    "        try:\n",
    "            # Initialize TF-IDF vectorizer\n",
    "            logging.info(\"Initializing TF-IDF vectorizer\")\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                max_features=1000,  # Limit to top 1000 features\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "                min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "                max_df=0.95  # Ignore terms that appear in more than 95% of documents\n",
    "            )\n",
    "            \n",
    "            # Fit and transform the comments\n",
    "            logging.info(\"Fitting and transforming comments with TF-IDF\")\n",
    "            self.feature_matrix = self.vectorizer.fit_transform(valid_comments)\n",
    "            logging.info(f\"Feature matrix created with shape: {self.feature_matrix.shape}\")\n",
    "            \n",
    "            return self.feature_matrix\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in vectorize_comments: {e}\")\n",
    "            logging.error(f\"Valid comments sample: {valid_comments[:5]}\")\n",
    "            raise\n",
    "    \n",
    "    def find_optimal_clusters(self, feature_matrix: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Find optimal number of clusters using elbow method and silhouette analysis.\n",
    "        \n",
    "        Args:\n",
    "            feature_matrix: TF-IDF feature matrix\n",
    "            \n",
    "        Returns:\n",
    "            Optimal number of clusters\n",
    "        \"\"\"\n",
    "        logging.info(f\"find_optimal_clusters called with feature_matrix shape: {feature_matrix.shape}\")\n",
    "        \n",
    "        try:\n",
    "            max_k = min(self.max_clusters, feature_matrix.shape[0] - 1)\n",
    "            logging.info(f\"max_k calculated as: {max_k}\")\n",
    "            \n",
    "            if max_k < self.min_clusters:\n",
    "                logging.warning(f\"Not enough comments for clustering. Using {max_k} clusters.\")\n",
    "                print(f\"Warning: Not enough comments for clustering. Using {max_k} clusters.\")\n",
    "                return max_k\n",
    "            \n",
    "            inertias = []\n",
    "            silhouette_scores = []\n",
    "            k_range = range(self.min_clusters, max_k + 1)\n",
    "            logging.info(f\"Testing k values: {list(k_range)}\")\n",
    "            \n",
    "            for k in k_range:\n",
    "                logging.debug(f\"Testing k={k}\")\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)\n",
    "                    logging.debug(f\"Created KMeans object for k={k}\")\n",
    "                    \n",
    "                    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "                    logging.debug(f\"fit_predict completed for k={k}, labels shape: {cluster_labels.shape}\")\n",
    "                    \n",
    "                    inertias.append(kmeans.inertia_)\n",
    "                    logging.debug(f\"Inertia for k={k}: {kmeans.inertia_}\")\n",
    "                    \n",
    "                    if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette score\n",
    "                        logging.debug(f\"Calculating silhouette score for k={k}\")\n",
    "                        sil_score = silhouette_score(feature_matrix, cluster_labels)\n",
    "                        silhouette_scores.append(sil_score)\n",
    "                        logging.debug(f\"Silhouette score for k={k}: {sil_score}\")\n",
    "                    else:\n",
    "                        silhouette_scores.append(0)\n",
    "                        logging.debug(f\"Only one cluster for k={k}, using silhouette score 0\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error testing k={k}: {e}\")\n",
    "                    raise\n",
    "            \n",
    "            logging.info(\"Finding elbow point\")\n",
    "            # Find elbow point\n",
    "            optimal_k = self._find_elbow_point(k_range, inertias)\n",
    "            logging.info(f\"Elbow point found at k={optimal_k}\")\n",
    "            \n",
    "            # Validate with silhouette score\n",
    "            if silhouette_scores:\n",
    "                best_sil_k = k_range[np.argmax(silhouette_scores)]\n",
    "                logging.info(f\"Best silhouette score at k={best_sil_k}\")\n",
    "                print(f\"Elbow method suggests {optimal_k} clusters\")\n",
    "                print(f\"Best silhouette score at {best_sil_k} clusters ({max(silhouette_scores):.3f})\")\n",
    "                \n",
    "                # Use silhouette score if significantly better\n",
    "                if max(silhouette_scores) > 0.3 and abs(best_sil_k - optimal_k) <= 2:\n",
    "                    optimal_k = best_sil_k\n",
    "                    logging.info(f\"Using silhouette-based optimal k={optimal_k}\")\n",
    "            \n",
    "            return optimal_k\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in find_optimal_clusters: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _find_elbow_point(self, k_range: range, inertias: List[float]) -> int:\n",
    "        \"\"\"Find the elbow point in the inertia curve.\"\"\"\n",
    "        logging.debug(f\"_find_elbow_point called with k_range: {list(k_range)}, inertias: {inertias}\")\n",
    "        \n",
    "        try:\n",
    "            if len(inertias) < 3:\n",
    "                logging.debug(\"Less than 3 inertias, returning first k value\")\n",
    "                return list(k_range)[0]\n",
    "            \n",
    "            # Calculate the rate of change\n",
    "            logging.debug(\"Calculating rate of change\")\n",
    "            changes = []\n",
    "            for i in range(1, len(inertias)):\n",
    "                change = inertias[i-1] - inertias[i]\n",
    "                changes.append(change)\n",
    "                logging.debug(f\"Change {i}: {change}\")\n",
    "            \n",
    "            # Find the point where the rate of change decreases most\n",
    "            if len(changes) > 1:\n",
    "                logging.debug(\"Calculating second-order changes\")\n",
    "                second_changes = []\n",
    "                for i in range(1, len(changes)):\n",
    "                    second_change = changes[i-1] - changes[i]\n",
    "                    second_changes.append(second_change)\n",
    "                    logging.debug(f\"Second change {i}: {second_change}\")\n",
    "                \n",
    "                if second_changes:\n",
    "                    elbow_idx = np.argmax(second_changes) + 2  # +2 because of indexing offset\n",
    "                    k_range_list = list(k_range)\n",
    "                    result_idx = min(elbow_idx, len(k_range_list) - 1)\n",
    "                    result = k_range_list[result_idx]\n",
    "                    logging.debug(f\"Elbow point found at index {elbow_idx}, returning k={result}\")\n",
    "                    return result\n",
    "            \n",
    "            # Fallback: use middle of range\n",
    "            k_range_list = list(k_range)\n",
    "            fallback_idx = len(k_range_list) // 2\n",
    "            result = k_range_list[fallback_idx]\n",
    "            logging.debug(f\"Using fallback middle value: k={result}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in _find_elbow_point: {e}\")\n",
    "            logging.error(f\"k_range: {k_range} (type: {type(k_range)})\")\n",
    "            logging.error(f\"inertias: {inertias} (type: {type(inertias)})\")\n",
    "            raise\n",
    "    \n",
    "    def cluster_comments(self, feature_matrix: np.ndarray, n_clusters: Optional[int] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform K-means clustering on the feature matrix.\n",
    "        \n",
    "        Args:\n",
    "            feature_matrix: TF-IDF feature matrix\n",
    "            n_clusters: Number of clusters (auto-detected if None)\n",
    "            \n",
    "        Returns:\n",
    "            Cluster labels\n",
    "        \"\"\"\n",
    "        logging.info(f\"cluster_comments called with feature_matrix shape: {feature_matrix.shape}\")\n",
    "        \n",
    "        try:\n",
    "            if n_clusters is None:\n",
    "                logging.info(\"Finding optimal number of clusters\")\n",
    "                n_clusters = self.find_optimal_clusters(feature_matrix)\n",
    "            \n",
    "            logging.info(f\"Using {n_clusters} clusters\")\n",
    "            self.optimal_k = n_clusters\n",
    "            \n",
    "            # Perform K-means clustering\n",
    "            logging.info(\"Initializing K-means\")\n",
    "            self.kmeans = KMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                n_init=10,\n",
    "                max_iter=300\n",
    "            )\n",
    "            \n",
    "            logging.info(\"Fitting K-means and predicting clusters\")\n",
    "            self.cluster_labels = self.kmeans.fit_predict(feature_matrix)\n",
    "            logging.info(f\"Clustering complete. Labels shape: {self.cluster_labels.shape}\")\n",
    "            \n",
    "            return self.cluster_labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in cluster_comments: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def extract_topic_keywords(self, n_keywords: int = 10) -> Dict[int, List[str]]:\n",
    "        \"\"\"\n",
    "        Extract top keywords for each cluster to understand topics.\n",
    "        \n",
    "        Args:\n",
    "            n_keywords: Number of top keywords per cluster\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping cluster ID to list of keywords\n",
    "        \"\"\"\n",
    "        logging.info(f\"extract_topic_keywords called with n_keywords={n_keywords}\")\n",
    "        \n",
    "        if self.kmeans is None or self.vectorizer is None:\n",
    "            raise ValueError(\"Must run clustering before extracting keywords\")\n",
    "        \n",
    "        try:\n",
    "            logging.info(\"Getting feature names from vectorizer\")\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            logging.debug(f\"Feature names type: {type(feature_names)}, sample: {feature_names[:10] if len(feature_names) > 0 else 'empty'}\")\n",
    "            \n",
    "            logging.info(\"Getting cluster centers\")\n",
    "            cluster_centers = self.kmeans.cluster_centers_\n",
    "            logging.info(f\"Cluster centers shape: {cluster_centers.shape}\")\n",
    "            \n",
    "            for cluster_id in range(len(cluster_centers)):\n",
    "                logging.debug(f\"Processing cluster {cluster_id}\")\n",
    "                # Get top feature indices for this cluster\n",
    "                top_indices = cluster_centers[cluster_id].argsort()[-n_keywords:][::-1]\n",
    "                logging.debug(f\"Top indices for cluster {cluster_id}: {top_indices}\")\n",
    "                \n",
    "                top_keywords = []\n",
    "                for i in top_indices:\n",
    "                    keyword = feature_names[i]\n",
    "                    logging.debug(f\"  Feature {i}: {repr(keyword)} (type: {type(keyword)})\")\n",
    "                    top_keywords.append(keyword)\n",
    "                \n",
    "                self.topic_keywords[cluster_id] = top_keywords\n",
    "                logging.debug(f\"Keywords for cluster {cluster_id}: {top_keywords}\")\n",
    "            \n",
    "            logging.info(f\"Topic keywords extraction complete: {len(self.topic_keywords)} clusters\")\n",
    "            return self.topic_keywords\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in extract_topic_keywords: {e}\")\n",
    "            logging.error(f\"kmeans: {self.kmeans}\")\n",
    "            logging.error(f\"vectorizer: {self.vectorizer}\")\n",
    "            raise\n",
    "    \n",
    "    def create_results_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create results DataFrame with cluster assignments and topic keywords.\n",
    "        \n",
    "        Args:\n",
    "            df: Original comments DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with cluster information\n",
    "        \"\"\"\n",
    "        results_df = df.copy()\n",
    "        \n",
    "        # Add cluster assignments\n",
    "        if len(self.cluster_labels) == len(df):\n",
    "            results_df['cluster_id'] = self.cluster_labels\n",
    "        else:\n",
    "            # Handle case where some comments were filtered out\n",
    "            results_df['cluster_id'] = -1\n",
    "            valid_idx = 0\n",
    "            for idx, row in results_df.iterrows():\n",
    "                if row['comment_text'] and row['comment_text'].strip():\n",
    "                    if valid_idx < len(self.cluster_labels):\n",
    "                        results_df.loc[idx, 'cluster_id'] = self.cluster_labels[valid_idx]\n",
    "                        valid_idx += 1\n",
    "        \n",
    "        # Add topic keywords\n",
    "        results_df['topic_keywords'] = results_df['cluster_id'].apply(\n",
    "            lambda x: ', '.join(self.topic_keywords.get(x, ['unknown'])) if x >= 0 else 'filtered'\n",
    "        )\n",
    "        \n",
    "        # Add preprocessed text for reference\n",
    "        results_df['preprocessed_text'] = results_df['comment_text'].apply(self.preprocess_text)\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def export_results(self, results_df: pd.DataFrame, output_file: str):\n",
    "        \"\"\"\n",
    "        Export clustering results to CSV.\n",
    "        \n",
    "        Args:\n",
    "            results_df: Results DataFrame\n",
    "            output_file: Output CSV file path\n",
    "        \"\"\"\n",
    "        # Reorder columns for better readability\n",
    "        column_order = [\n",
    "            'cluster_id', 'topic_keywords', 'comment_text', 'preprocessed_text',\n",
    "            'author', 'score', 'depth', 'created_utc', 'comment_id', 'parent_id', 'original_index'\n",
    "        ]\n",
    "        \n",
    "        # Only include columns that exist\n",
    "        available_columns = [col for col in column_order if col in results_df.columns]\n",
    "        export_df = results_df[available_columns]\n",
    "        \n",
    "        export_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\nClustering Results Summary:\")\n",
    "        print(f\"Total comments processed: {len(results_df)}\")\n",
    "        print(f\"Number of clusters: {self.optimal_k}\")\n",
    "        print(f\"Results exported to: {output_file}\")\n",
    "        \n",
    "        cluster_counts = results_df['cluster_id'].value_counts().sort_index()\n",
    "        print(f\"\\nComments per cluster:\")\n",
    "        for cluster_id, count in cluster_counts.items():\n",
    "            if cluster_id >= 0:\n",
    "                keywords = ', '.join(self.topic_keywords.get(cluster_id, ['unknown'])[:5])\n",
    "                print(f\"  Cluster {cluster_id}: {count} comments (Keywords: {keywords})\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to handle command line arguments and execute clustering.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Cluster Reddit comments by topics using unsupervised learning'\n",
    "    )\n",
    "    parser.add_argument('input_csv', help='Input CSV file from reddit_nested_comments_scraper.py')\n",
    "    parser.add_argument('-o', '--output', default='reddit_comments_clustered.csv',\n",
    "                       help='Output CSV file path (default: reddit_comments_clustered.csv)')\n",
    "    parser.add_argument('-k', '--clusters', type=int,\n",
    "                       help='Number of clusters (auto-detected if not specified)')\n",
    "    parser.add_argument('--min-clusters', type=int, default=2,\n",
    "                       help='Minimum number of clusters to try (default: 2)')\n",
    "    parser.add_argument('--max-clusters', type=int, default=15,\n",
    "                       help='Maximum number of clusters to try (default: 15)')\n",
    "    parser.add_argument('--keywords', type=int, default=10,\n",
    "                       help='Number of keywords to extract per topic (default: 10)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading comments from: {args.input_csv}\")\n",
    "        \n",
    "        # Initialize clusterer\n",
    "        clusterer = RedditCommentClusterer(\n",
    "            min_clusters=args.min_clusters,\n",
    "            max_clusters=args.max_clusters\n",
    "        )\n",
    "        \n",
    "        # Load and preprocess comments\n",
    "        df = clusterer.load_comments_from_csv(args.input_csv)\n",
    "        \n",
    "        if len(df) < 2:\n",
    "            print(\"Error: Need at least 2 comments for clustering.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} comments\")\n",
    "        \n",
    "        # Preprocess and vectorize\n",
    "        logging.info(\"Starting text preprocessing\")\n",
    "        preprocessed_comments = []\n",
    "        for i, text in enumerate(df['comment_text']):\n",
    "            logging.debug(f\"Preprocessing comment {i}: {repr(text)}\")\n",
    "            processed = clusterer.preprocess_text(text)\n",
    "            preprocessed_comments.append(processed)\n",
    "        \n",
    "        logging.info(f\"Preprocessing complete. Got {len(preprocessed_comments)} processed comments\")\n",
    "        feature_matrix = clusterer.vectorize_comments(preprocessed_comments)\n",
    "        \n",
    "        print(f\"Created feature matrix with {feature_matrix.shape[1]} features\")\n",
    "        \n",
    "        # Perform clustering\n",
    "        cluster_labels = clusterer.cluster_comments(feature_matrix, args.clusters)\n",
    "        \n",
    "        # Extract topic keywords\n",
    "        topic_keywords = clusterer.extract_topic_keywords(args.keywords)\n",
    "        \n",
    "        # Create and export results\n",
    "        results_df = clusterer.create_results_dataframe(df)\n",
    "        clusterer.export_results(results_df, args.output)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
